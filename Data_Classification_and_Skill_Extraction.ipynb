{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from time import time\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as pl\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "import scipy as sp\n",
    "from sklearn import random_projection\n",
    "from sklearn.externals import joblib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk import ngrams\n",
    "from google.cloud import translate\n",
    "import urllib\n",
    "import time\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BASIC PREPROCESSING\n",
    "def basic_cleaner(text, language=None, skill_match = None):\n",
    "    \"\"\"\n",
    "    Purpose of this function to clean the appropriate texts \n",
    "    with language specific stemming\n",
    "    \"\"\"\n",
    "    \n",
    "    if language == None or language=='en':\n",
    "        stemmer = PorterStemmer()\n",
    "    elif language == 'sv':\n",
    "        stemmer = SnowballStemmer(\"swedish\")\n",
    "    elif language == 'da':\n",
    "        stemmer = SnowballStemmer(\"danish\")\n",
    "    elif language == 'fi':\n",
    "        stemmer = SnowballStemmer(\"finnish\")\n",
    "        \n",
    "    if skill_match == None:\n",
    "        if language == None or language=='en':\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        else:\n",
    "            text = re.sub(r'[^\\w]', ' ', text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = wpt.tokenize(text)\n",
    "    # filter stopwords out of document\n",
    "    text = [stemmer.stem(token) for token in text]\n",
    "    # re-create document from filtered tokens\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def english_main_classifier(dataframe, job_title, job_desc):\n",
    "    \"\"\"\n",
    "    Purpose of this function to classify english job roles to the ESCO taxonomy\n",
    "    \"\"\"\n",
    "    #Preprocessing\n",
    "    print('cleaning of texts started : {}'.format(datetime.datetime.now()))\n",
    "    dataframe[job_title + '_cln'] = dataframe[job_title].apply(basic_cleaner)\n",
    "    dataframe[job_desc + '_cln'] = dataframe[job_desc].apply(basic_cleaner)\n",
    "    print('cleaning of texts finished : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    #Generating features from job roles - loading already fitted tfidf\n",
    "    print('vectorization, and random projection started : {}'.format(datetime.datetime.now()))\n",
    "    tfidf_role = joblib.load('tf_idf_role')\n",
    "    tf_idf_res_role = tfidf_role.transform(dataframe[job_title + '_cln'])\n",
    "    \n",
    "    #Generating features from job descriptions - loading already fitted tfidf\n",
    "    tfidf_desc = joblib.load('tf_idf_desc')\n",
    "    tf_idf_res_desc = tfidf_desc.transform(dataframe[job_desc + '_cln'])\n",
    "    \n",
    "    #applying Random Projection, an unsupervised learning method for dimensionality reduction\n",
    "    rp = joblib.load('rp')\n",
    "    tfidf_desc_rand_proj = rp.transform(tf_idf_res_desc)\n",
    "    \n",
    "    #combining feature matrixes\n",
    "    tf_idf_final = sp.sparse.hstack((tf_idf_res_role, tfidf_desc_rand_proj))\n",
    "    \n",
    "    print('vectorization, and random projection finished : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    #loading trained SGDClassifier\n",
    "    print('classification started : {}'.format(datetime.datetime.now()))\n",
    "    clf = joblib.load('english_classifier_joblib')\n",
    "    label_encoder = joblib.load('label_encoder')\n",
    "    \n",
    "    prediction = clf.predict(tf_idf_final)\n",
    "    prediction = pd.DataFrame(label_encoder.inverse_transform(prediction),\n",
    "                              columns=['prediction'])\n",
    "    \n",
    "    \n",
    "    #Assigning probability to the respective predictions\n",
    "    predicted_prob = pd.DataFrame(clf.predict_proba(tf_idf_final))\n",
    "    predicted_prob['max_prob'] = predicted_prob.apply(lambda prob: max(prob), axis=1)\n",
    "    \n",
    "    dataframe.reset_index(inplace=True)\n",
    "    del(dataframe['index'])\n",
    "    \n",
    "    #Finalizing results\n",
    "    dataframe = pd.merge(dataframe, prediction, how='left', left_index=True, right_index=True)\n",
    "    dataframe = pd.merge(dataframe, pd.DataFrame(predicted_prob['max_prob']), \n",
    "                         how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    print('classification finished : {}'.format(datetime.datetime.now()))\n",
    "    return dataframe\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_language(text):\n",
    "    \"\"\"\n",
    "    Purpose of this function to detect the foreing languages \n",
    "    in the text\n",
    "    \"\"\"\n",
    "    lang = translate_client.detect_language(text)\n",
    "    time.sleep(1)\n",
    "    return lang['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_text(text, language):\n",
    "    \"\"\"\n",
    "    Function for using Cloud Based Google translate\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    translation = translate_client.translate(\n",
    "        text,\n",
    "        source_language=language,\n",
    "        target_language='en')\n",
    "\n",
    "    return translation['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_lang_classifier(dataframe, job_title, job_desc, language):\n",
    "    \"\"\"\n",
    "    Purpose of this function to classify non english job roles to ESCO taxonomy\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #translating foreign positions to english\n",
    "    positions = pd.DataFrame(dataframe[job_title].drop_duplicates())\n",
    "    print('translation started : {}'.format(datetime.datetime.now()))\n",
    "    positions[job_title + '_translated'] = positions[job_title].apply(translate_text, language = language)\n",
    "    print('translation finished : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    \n",
    "    print('cleaning and vectorization started : {}'.format(datetime.datetime.now()))\n",
    "    dataframe = pd.merge(dataframe, positions, on=job_title, how='left')\n",
    "    \n",
    "    #Cleaning and creating feature matrixes\n",
    "    dataframe[job_title +'_translated_cln'] = dataframe[job_title + '_translated'].apply(basic_cleaner)\n",
    "    \n",
    "    tf_idf_role = joblib.load('multi_lang_tfidf')\n",
    "    tf_idf_final = tf_idf_role.transform(dataframe[job_title +'_translated_cln'])\n",
    "    print('cleaning and vectorization finished : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    #Prediction with english trained model on now translated positions\n",
    "    print('classification started : {}'.format(datetime.datetime.now()))\n",
    "    clf = joblib.load('multi_lang_classifier_joblib')\n",
    "    label_encoder = joblib.load('label_encoder')\n",
    "    \n",
    "    prediction = clf.predict(tf_idf_final)\n",
    "    prediction = pd.DataFrame(label_encoder.inverse_transform(prediction), columns=['prediction'])\n",
    "    \n",
    "    \n",
    "    #assigning probabilities and joining together results\n",
    "    predicted_prob = pd.DataFrame(clf.predict_proba(tf_idf_final))\n",
    "    predicted_prob['max_prob'] = predicted_prob.apply(lambda prob: max(prob), axis=1)\n",
    "    \n",
    "    dataframe.reset_index(inplace=True)\n",
    "    del(dataframe['index'])\n",
    "    \n",
    "    \n",
    "    dataframe = pd.merge(dataframe, prediction, how='left', left_index=True, right_index=True)\n",
    "    dataframe = pd.merge(dataframe, pd.DataFrame(predicted_prob['max_prob']), how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    print('classification finished : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_skills(dataframe, description, \n",
    "                 job_id, skills_one_token,\n",
    "                 skills_two_token, skills_three_token, \n",
    "                 skills_four_token):\n",
    "    \n",
    "    #Matching ESCO skills with same token length\n",
    "    unigram = ngrams(dataframe[description].split(), 1)\n",
    "    unigram = set([' '.join(grams) for grams in unigram])\n",
    "    bigram = set(ngrams(dataframe[description].split(), 2))\n",
    "    bigram = set([' '.join(grams) for grams in bigram])\n",
    "    trigram = set(ngrams(dataframe[description].split(), 3))\n",
    "    trigram = set([' '.join(grams) for grams in trigram])\n",
    "    fourgram = set(ngrams(dataframe[description].split(), 4))\n",
    "    fourgram = set([' '.join(grams) for grams in fourgram])\n",
    "\n",
    "    common_unigram= list(unigram.intersection(skills_one_token))\n",
    "    common_bigram = list(bigram.intersection(skills_two_token))\n",
    "    common_trigram = list(trigram.intersection(skills_three_token))\n",
    "    common_fourgram = list(fourgram.intersection(skills_four_token))\n",
    "    \n",
    "    all_skill = [common_unigram, common_bigram, common_trigram, common_fourgram]\n",
    "\n",
    "    all_skill = [[dataframe[job_id], item] for sublist in all_skill for item in sublist]\n",
    "\n",
    "    if len(common_unigram) >= 1 or len(common_bigram) >= 1 or len(common_trigram) >= 1 or len(common_fourgram) >=1:\n",
    "        return all_skill\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_skills_base(dataframe,job_id, job_desc, language):\n",
    "    \n",
    "    ###put in language specific cleaning for descriptions\n",
    "    print('cleaning started : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    dataframe[job_desc + '_cln'] = dataframe[job_desc].apply(basic_cleaner, language=language, skill_match=True)\n",
    "    print('cleaning finished : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    print('skill preparation started : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    skills_need= pd.read_excel('skills_check_04.xlsx')\n",
    "    skills_need = skills_need[skills_need['need']==1]\n",
    "    \n",
    "    if language=='en':\n",
    "        skills_need_list = pd.DataFrame(list(skills_need['preferred_skill_label']), columns=['skill_desc'])\n",
    "    else:\n",
    "        skills_need_list = pd.DataFrame(list(skills_need['preferred_skill_label_' + language]), columns=['skill_desc'])\n",
    "    \n",
    "    \n",
    "    skills_need_list.fillna('mv-9', inplace=True)\n",
    "    \n",
    "    skills_need_list = skills_need_list[skills_need_list['skill_desc']!='mv-9'].copy()\n",
    "    \n",
    "    skills_need_list['skill_desc_cln'] = skills_need_list['skill_desc'].apply(basic_cleaner, language=language, skill_match=True)\n",
    "    \n",
    "    skills_need_list['token_count'] = skills_need_list['skill_desc_cln'].apply(lambda text: len(wpt.tokenize(text)))\n",
    "    \n",
    "    skills_four_token = skills_need_list['skill_desc_cln'][skills_need_list['token_count']==4].copy()\n",
    "    \n",
    "    skills_three_token = skills_need_list['skill_desc_cln'][skills_need_list['token_count']==3].copy()\n",
    "    \n",
    "    skills_two_token = skills_need_list['skill_desc_cln'][skills_need_list['token_count']==2].copy()\n",
    "    \n",
    "    skills_one_token = skills_need_list['skill_desc_cln'][skills_need_list['token_count']==1].copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('skill preparation finished : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    all_skills_complete=[]\n",
    "    \n",
    "    \n",
    "    print('skill matching started : {}'.format(datetime.datetime.now()))\n",
    "    all_skills_complete.append(dataframe.apply(match_skills, description=job_desc + '_cln' , \n",
    "                                               job_id=job_id,\n",
    "                                               skills_one_token=skills_one_token,\n",
    "                                               skills_two_token=skills_two_token,\n",
    "                                               skills_three_token=skills_three_token,\n",
    "                                               skills_four_token=skills_four_token,\n",
    "                                               axis=1))\n",
    "    print('skill matching finished : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    print('finalization  started : {}'.format(datetime.datetime.now()))\n",
    "    flat_list = [item for sublist in all_skills_complete for item in sublist]\n",
    "    flat_list = [item for sublist in flat_list for item in sublist]\n",
    "    \n",
    "    all_skills_complete = pd.DataFrame(flat_list, columns=[job_id, 'skill_desc_cln'])\n",
    "    \n",
    "    all_skills_complete = pd.merge(all_skills_complete, skills_need_list[['skill_desc', 'skill_desc_cln']],\n",
    "                               on='skill_desc_cln', how='left')\n",
    "    if language == 'en':\n",
    "        all_skills_complete.rename(columns={'skill_desc' : 'preferred_skill_label'},  inplace=True)\n",
    "        all_skills_complete = pd.merge(all_skills_complete, skills_need[['preferred_skill_label', 'skillUri']],\n",
    "                               on='preferred_skill_label', how='left')\n",
    "        \n",
    "    else:\n",
    "        all_skills_complete.rename(columns={'skill_desc' : 'preferred_skill_label_' + language},  inplace=True)\n",
    "    \n",
    "        all_skills_complete = pd.merge(all_skills_complete, skills_need[['preferred_skill_label_' + language,\n",
    "                                                                     'preferred_skill_label',\n",
    "                                                                     'skillUri']],\n",
    "                               on='preferred_skill_label_' + language, how='left')\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    data_skill_matched = pd.merge(dataframe, all_skills_complete, on=job_id, how='left')\n",
    "    print('finalization  finished : {}'.format(datetime.datetime.now()))\n",
    "    \n",
    "    return data_skill_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def swedish_location_correcter(df):\n",
    "    if df['text3'] == 'mv-9':\n",
    "        return df['text2']\n",
    "    else:\n",
    "        return df['text3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading language specific stopwords for multilingual matching\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words_en = nltk.corpus.stopwords.words('english')\n",
    "stop_words_sv = nltk.corpus.stopwords.words('swedish')\n",
    "stop_words_fi = nltk.corpus.stopwords.words('finnish')\n",
    "\n",
    "stop_words_en += ['job', 'title', 'position', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load credentials for cloud based Google Translate\n",
    "credentials = service_account.Credentials.from_service_account_file(\"My First Project-fab01784b0d3.json\")\n",
    "scoped_credentials = credentials.with_scopes(\n",
    "    ['https://www.googleapis.com/auth/cloud-platform'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instantiating translator\n",
    "translate_client = translate.Client(credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in input data\n",
    "data_fi = pd.read_excel('FI_FULL.xlsx')\n",
    "data_sv = pd.read_excel('SE_FULL2018-10-30.xlsx')\n",
    "data_dk = pd.read_excel('DA_FULL_2018_11_01.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#swedish data preparation\n",
    "splitted = data_sv['company'].str.split('-', expand=True)\n",
    "splitted.columns=['text1', 'text2', 'text3']\n",
    "splitted.fillna('mv-9', inplace=True)\n",
    "splitted['location'] = splitted.apply(swedish_location_correcter, axis=1)\n",
    "data_sv = pd.merge(data_sv, pd.DataFrame(splitted['location']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Joining multi language data\n",
    "data = pd.concat([data_fi, data_sv, data_dk], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#assigning unique id for foles\n",
    "data['job_id'] = data['position'] + ':' + data['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#handling NULL values\n",
    "data.fillna('mv-9', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rationalising description length for language identification\n",
    "data['description_reduced'] = data['description'].apply(lambda text: text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang identification started : 2018-11-01 15:41:19.600228\n",
      "lang identification finished : 2018-11-01 16:18:28.565160\n"
     ]
    }
   ],
   "source": [
    "#calling language identifier API\n",
    "print('lang identification started : {}'.format(datetime.datetime.now()))\n",
    "data['language'] = data['description_reduced'].apply(identify_language)\n",
    "print('lang identification finished : {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load in validated language identification results\n",
    "data = pd.read_excel('datamodel_check_lang_detection_2018_11_01_v2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unique list of languages in data\n",
    "languages = list(data['language'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current language fi\n",
      "translation started : 2018-11-01 17:25:37.630153\n",
      "translation finished : 2018-11-01 17:26:47.799157\n",
      "cleaning and vectorization started : 2018-11-01 17:26:47.800156\n",
      "cleaning and vectorization finished : 2018-11-01 17:26:48.007573"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cameg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "classification started : 2018-11-01 17:26:48.007573\n",
      "classification finished : 2018-11-01 17:26:48.216744\n",
      "current language en\n",
      "cleaning of texts started : 2018-11-01 17:26:48.222729\n",
      "cleaning of texts finished : 2018-11-01 17:26:54.750623\n",
      "vectorization, and random projection started : 2018-11-01 17:26:54.750623\n",
      "vectorization, and random projection finished : 2018-11-01 17:27:27.030952\n",
      "classification started : 2018-11-01 17:27:27.031951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cameg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification finished : 2018-11-01 17:27:27.398056\n",
      "current language da\n",
      "translation started : 2018-11-01 17:27:27.645904\n",
      "translation finished : 2018-11-01 17:29:06.744565\n",
      "cleaning and vectorization started : 2018-11-01 17:29:06.745553\n",
      "cleaning and vectorization finished : 2018-11-01 17:29:06.954422\n",
      "classification started : 2018-11-01 17:29:06.954422\n",
      "classification finished : 2018-11-01 17:29:06.973421\n",
      "current language sv\n",
      "translation started : 2018-11-01 17:29:06.983415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cameg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translation finished : 2018-11-01 17:35:09.167860\n",
      "cleaning and vectorization started : 2018-11-01 17:35:09.167860\n",
      "cleaning and vectorization finished : 2018-11-01 17:35:09.454682\n",
      "classification started : 2018-11-01 17:35:09.454682\n",
      "classification finished : 2018-11-01 17:35:09.493658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cameg\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "###Language specific classification process\n",
    "class_all = pd.DataFrame()\n",
    "\n",
    "for lang in languages:\n",
    "    print('current language {}'.format(lang))    \n",
    "\n",
    "    if lang == 'en':\n",
    "        data_need = data[data['language']=='en'].copy()\n",
    "        data_class = english_main_classifier(data_need, 'position', 'description')\n",
    "        class_all = pd.concat([class_all, data_class])\n",
    "    else:\n",
    "        data_need = data[data['language']==lang].copy()\n",
    "        data_class = multi_lang_classifier(data_need, 'position', 'description', lang)\n",
    "        class_all = pd.concat([class_all, data_class])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class_all.to_excel('live_demo_01.xlsx')\n",
    "#class_all = pd.read_excel('process_check_01.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skill_matched_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fi\n",
      "cleaning started : 2018-11-01 17:57:35.276916\n",
      "cleaning finished : 2018-11-01 17:57:37.645506\n",
      "skill preparation started : 2018-11-01 17:57:37.645506\n",
      "skill preparation finished : 2018-11-01 17:57:37.732459\n",
      "skill matching started : 2018-11-01 17:57:37.732459\n",
      "skill matching finished : 2018-11-01 17:57:38.294141\n",
      "finalization  started : 2018-11-01 17:57:38.294141\n",
      "finalization  finished : 2018-11-01 17:57:38.313758\n",
      "en\n",
      "cleaning started : 2018-11-01 17:57:38.316756\n",
      "cleaning finished : 2018-11-01 17:57:44.626870\n",
      "skill preparation started : 2018-11-01 17:57:44.626870\n",
      "skill preparation finished : 2018-11-01 17:57:44.715810\n",
      "skill matching started : 2018-11-01 17:57:44.715810\n",
      "skill matching finished : 2018-11-01 17:57:45.880092\n",
      "finalization  started : 2018-11-01 17:57:45.880092\n",
      "finalization  finished : 2018-11-01 17:57:45.893085\n",
      "da\n",
      "cleaning started : 2018-11-01 17:57:45.899081\n",
      "cleaning finished : 2018-11-01 17:57:47.493099\n",
      "skill preparation started : 2018-11-01 17:57:47.493099\n",
      "skill preparation finished : 2018-11-01 17:57:47.578047\n",
      "skill matching started : 2018-11-01 17:57:47.578047\n",
      "skill matching finished : 2018-11-01 17:57:48.020775\n",
      "finalization  started : 2018-11-01 17:57:48.020775\n",
      "finalization  finished : 2018-11-01 17:57:48.030768\n",
      "sv\n",
      "cleaning started : 2018-11-01 17:57:48.038763\n",
      "cleaning finished : 2018-11-01 17:57:53.898047\n",
      "skill preparation started : 2018-11-01 17:57:53.898047\n",
      "skill preparation finished : 2018-11-01 17:57:53.982001\n",
      "skill matching started : 2018-11-01 17:57:53.983006\n",
      "skill matching finished : 2018-11-01 17:57:55.997754\n",
      "finalization  started : 2018-11-01 17:57:55.997754\n",
      "finalization  finished : 2018-11-01 17:57:56.010747\n"
     ]
    }
   ],
   "source": [
    "#Language specific skill extraction from job descriptions\n",
    "for lang in languages:\n",
    "    print(lang)\n",
    "    data_need = class_all[class_all['language']==lang].copy()\n",
    "    data_skill_matched = match_skills_base(data_need, 'job_id', 'description', lang)\n",
    "    skill_matched_all = pd.concat([skill_matched_all, data_skill_matched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extract processed and enriched data\n",
    "skill_matched_all.to_excel('live_demo_full_01.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
